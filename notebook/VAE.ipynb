{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.functional import jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)     # Outputs the mean\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim) # Outputs the log variance\n",
    "\n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encodes input by mapping it into the latent space.\"\"\"\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h1)\n",
    "        logvar = self.fc_logvar(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Applies the reparameterization trick to sample from N(mu, var).\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)   # Calculate the standard deviation\n",
    "        eps = torch.randn_like(std)     # Sample from standard normal\n",
    "        return mu + eps * std           # Sample from N(mu, var)\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Decodes the latent representation z to reconstruct the input.\"\"\"\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        # x_recon = torch.sigmoid(self.fc4(h3))\n",
    "        x_recon = self.fc4(h3)\n",
    "\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the computation performed at every call.\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"Computes the VAE loss function.\"\"\"\n",
    "    # Reconstruction loss (binary cross entropy)\n",
    "    # BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "\n",
    "    # KL divergence between the approximate posterior and the prior\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) * 0.1\n",
    "\n",
    "    # KLD = 0\n",
    "\n",
    "    return MSE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n = 5000\n",
    "d = 10\n",
    "\n",
    "np.random.seed(seed)\n",
    "A1_vector = np.random.normal(0, 1, d)\n",
    "A2_vector = np.random.normal(0, 1, d)\n",
    "A1 = np.diag(A1_vector)\n",
    "A2 = np.diag(A2_vector)\n",
    "\n",
    "B1 = np.random.normal(0, 1, d)\n",
    "B2 = np.random.normal(0, 1, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "X = np.random.normal(0, 1, (n, d))\n",
    "\n",
    "Y1 = X * A1_vector + B1\n",
    "Y2 = X * A2_vector + B2\n",
    "\n",
    "Y = np.random.normal(0, 1, (n, d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = torch.from_numpy(Y1).float()\n",
    "Y2 = torch.from_numpy(Y2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 14.4744\n",
      "Epoch 2, Average Loss: 6.3806\n",
      "Epoch 3, Average Loss: 4.0787\n",
      "Epoch 4, Average Loss: 3.2962\n",
      "Epoch 5, Average Loss: 3.0169\n",
      "Epoch 6, Average Loss: 2.8513\n",
      "Epoch 7, Average Loss: 2.6544\n",
      "Epoch 8, Average Loss: 2.4790\n",
      "Epoch 9, Average Loss: 2.3632\n",
      "Epoch 10, Average Loss: 2.2644\n",
      "Epoch 11, Average Loss: 2.2217\n",
      "Epoch 12, Average Loss: 2.1924\n",
      "Epoch 13, Average Loss: 2.1677\n",
      "Epoch 14, Average Loss: 2.1385\n",
      "Epoch 15, Average Loss: 2.1153\n",
      "Epoch 16, Average Loss: 2.0865\n",
      "Epoch 17, Average Loss: 2.0704\n",
      "Epoch 18, Average Loss: 2.0459\n",
      "Epoch 19, Average Loss: 2.0320\n",
      "Epoch 20, Average Loss: 2.0149\n",
      "Epoch 21, Average Loss: 1.9999\n",
      "Epoch 22, Average Loss: 1.9994\n",
      "Epoch 23, Average Loss: 1.9834\n",
      "Epoch 24, Average Loss: 1.9813\n",
      "Epoch 25, Average Loss: 1.9714\n",
      "Epoch 26, Average Loss: 1.9643\n",
      "Epoch 27, Average Loss: 1.9580\n",
      "Epoch 28, Average Loss: 1.9475\n",
      "Epoch 29, Average Loss: 1.9442\n",
      "Epoch 30, Average Loss: 1.9362\n",
      "Epoch 31, Average Loss: 1.9352\n",
      "Epoch 32, Average Loss: 1.9297\n",
      "Epoch 33, Average Loss: 1.9142\n",
      "Epoch 34, Average Loss: 1.9088\n",
      "Epoch 35, Average Loss: 1.9069\n",
      "Epoch 36, Average Loss: 1.9057\n",
      "Epoch 37, Average Loss: 1.9073\n",
      "Epoch 38, Average Loss: 1.9031\n",
      "Epoch 39, Average Loss: 1.8979\n",
      "Epoch 40, Average Loss: 1.8884\n",
      "Epoch 41, Average Loss: 1.8864\n",
      "Epoch 42, Average Loss: 1.8919\n",
      "Epoch 43, Average Loss: 1.8887\n",
      "Epoch 44, Average Loss: 1.8809\n",
      "Epoch 45, Average Loss: 1.8817\n",
      "Epoch 46, Average Loss: 1.8742\n",
      "Epoch 47, Average Loss: 1.8690\n",
      "Epoch 48, Average Loss: 1.8707\n",
      "Epoch 49, Average Loss: 1.8767\n",
      "Epoch 50, Average Loss: 1.8690\n",
      "Epoch 51, Average Loss: 1.8590\n",
      "Epoch 52, Average Loss: 1.8652\n",
      "Epoch 53, Average Loss: 1.8576\n",
      "Epoch 54, Average Loss: 1.8538\n",
      "Epoch 55, Average Loss: 1.8606\n",
      "Epoch 56, Average Loss: 1.8571\n",
      "Epoch 57, Average Loss: 1.8555\n",
      "Epoch 58, Average Loss: 1.8543\n",
      "Epoch 59, Average Loss: 1.8547\n",
      "Epoch 60, Average Loss: 1.8584\n",
      "Epoch 61, Average Loss: 1.8478\n",
      "Epoch 62, Average Loss: 1.8512\n",
      "Epoch 63, Average Loss: 1.8399\n",
      "Epoch 64, Average Loss: 1.8462\n",
      "Epoch 65, Average Loss: 1.8442\n",
      "Epoch 66, Average Loss: 1.8496\n",
      "Epoch 67, Average Loss: 1.8411\n",
      "Epoch 68, Average Loss: 1.8458\n",
      "Epoch 69, Average Loss: 1.8445\n",
      "Epoch 70, Average Loss: 1.8393\n",
      "Epoch 71, Average Loss: 1.8377\n",
      "Epoch 72, Average Loss: 1.8403\n",
      "Epoch 73, Average Loss: 1.8414\n",
      "Epoch 74, Average Loss: 1.8328\n",
      "Epoch 75, Average Loss: 1.8294\n",
      "Epoch 76, Average Loss: 1.8376\n",
      "Epoch 77, Average Loss: 1.8423\n",
      "Epoch 78, Average Loss: 1.8314\n",
      "Epoch 79, Average Loss: 1.8434\n",
      "Epoch 80, Average Loss: 1.8404\n",
      "Epoch 81, Average Loss: 1.8347\n",
      "Epoch 82, Average Loss: 1.8300\n",
      "Epoch 83, Average Loss: 1.8306\n",
      "Epoch 84, Average Loss: 1.8277\n",
      "Epoch 85, Average Loss: 1.8316\n",
      "Epoch 86, Average Loss: 1.8314\n",
      "Epoch 87, Average Loss: 1.8311\n",
      "Epoch 88, Average Loss: 1.8305\n",
      "Epoch 89, Average Loss: 1.8274\n",
      "Epoch 90, Average Loss: 1.8283\n",
      "Epoch 91, Average Loss: 1.8342\n",
      "Epoch 92, Average Loss: 1.8298\n",
      "Epoch 93, Average Loss: 1.8257\n",
      "Epoch 94, Average Loss: 1.8252\n",
      "Epoch 95, Average Loss: 1.8247\n",
      "Epoch 96, Average Loss: 1.8331\n",
      "Epoch 97, Average Loss: 1.8286\n",
      "Epoch 98, Average Loss: 1.8279\n",
      "Epoch 99, Average Loss: 1.8232\n",
      "Epoch 100, Average Loss: 1.8256\n",
      "Epoch 101, Average Loss: 1.8219\n",
      "Epoch 102, Average Loss: 1.8238\n",
      "Epoch 103, Average Loss: 1.8226\n",
      "Epoch 104, Average Loss: 1.8273\n",
      "Epoch 105, Average Loss: 1.8146\n",
      "Epoch 106, Average Loss: 1.8278\n",
      "Epoch 107, Average Loss: 1.8350\n",
      "Epoch 108, Average Loss: 1.8216\n",
      "Epoch 109, Average Loss: 1.8232\n",
      "Epoch 110, Average Loss: 1.8262\n",
      "Epoch 111, Average Loss: 1.8249\n",
      "Epoch 112, Average Loss: 1.8250\n",
      "Epoch 113, Average Loss: 1.8222\n",
      "Epoch 114, Average Loss: 1.8236\n",
      "Epoch 115, Average Loss: 1.8184\n",
      "Epoch 116, Average Loss: 1.8252\n",
      "Epoch 117, Average Loss: 1.8288\n",
      "Epoch 118, Average Loss: 1.8222\n",
      "Epoch 119, Average Loss: 1.8223\n",
      "Epoch 120, Average Loss: 1.8198\n",
      "Epoch 121, Average Loss: 1.8263\n",
      "Epoch 122, Average Loss: 1.8281\n",
      "Epoch 123, Average Loss: 1.8257\n",
      "Epoch 124, Average Loss: 1.8183\n",
      "Epoch 125, Average Loss: 1.8276\n",
      "Epoch 126, Average Loss: 1.8235\n",
      "Epoch 127, Average Loss: 1.8204\n",
      "Epoch 128, Average Loss: 1.8211\n",
      "Epoch 129, Average Loss: 1.8194\n",
      "Epoch 130, Average Loss: 1.8225\n",
      "Epoch 131, Average Loss: 1.8216\n",
      "Epoch 132, Average Loss: 1.8209\n",
      "Epoch 133, Average Loss: 1.8201\n",
      "Epoch 134, Average Loss: 1.8208\n",
      "Epoch 135, Average Loss: 1.8273\n",
      "Epoch 136, Average Loss: 1.8204\n",
      "Epoch 137, Average Loss: 1.8171\n",
      "Epoch 138, Average Loss: 1.8154\n",
      "Epoch 139, Average Loss: 1.8197\n",
      "Epoch 140, Average Loss: 1.8266\n",
      "Epoch 141, Average Loss: 1.8169\n",
      "Epoch 142, Average Loss: 1.8150\n",
      "Epoch 143, Average Loss: 1.8172\n",
      "Epoch 144, Average Loss: 1.8208\n",
      "Epoch 145, Average Loss: 1.8154\n",
      "Epoch 146, Average Loss: 1.8254\n",
      "Epoch 147, Average Loss: 1.8201\n",
      "Epoch 148, Average Loss: 1.8228\n",
      "Epoch 149, Average Loss: 1.8187\n",
      "Epoch 150, Average Loss: 1.8179\n",
      "Epoch 151, Average Loss: 1.8209\n",
      "Epoch 152, Average Loss: 1.8143\n",
      "Epoch 153, Average Loss: 1.8210\n",
      "Epoch 154, Average Loss: 1.8122\n",
      "Epoch 155, Average Loss: 1.8216\n",
      "Epoch 156, Average Loss: 1.8142\n",
      "Epoch 157, Average Loss: 1.8136\n",
      "Epoch 158, Average Loss: 1.8165\n",
      "Epoch 159, Average Loss: 1.8203\n",
      "Epoch 160, Average Loss: 1.8190\n",
      "Epoch 161, Average Loss: 1.8136\n",
      "Epoch 162, Average Loss: 1.8131\n",
      "Epoch 163, Average Loss: 1.8137\n",
      "Epoch 164, Average Loss: 1.8176\n",
      "Epoch 165, Average Loss: 1.8058\n",
      "Epoch 166, Average Loss: 1.8146\n",
      "Epoch 167, Average Loss: 1.8136\n",
      "Epoch 168, Average Loss: 1.8186\n",
      "Epoch 169, Average Loss: 1.8071\n",
      "Epoch 170, Average Loss: 1.8106\n",
      "Epoch 171, Average Loss: 1.8171\n",
      "Epoch 172, Average Loss: 1.8081\n",
      "Epoch 173, Average Loss: 1.8094\n",
      "Epoch 174, Average Loss: 1.8058\n",
      "Epoch 175, Average Loss: 1.8139\n",
      "Epoch 176, Average Loss: 1.8098\n",
      "Epoch 177, Average Loss: 1.8075\n",
      "Epoch 178, Average Loss: 1.8038\n",
      "Epoch 179, Average Loss: 1.8179\n",
      "Epoch 180, Average Loss: 1.8056\n",
      "Epoch 181, Average Loss: 1.8080\n",
      "Epoch 182, Average Loss: 1.8114\n",
      "Epoch 183, Average Loss: 1.8114\n",
      "Epoch 184, Average Loss: 1.8066\n",
      "Epoch 185, Average Loss: 1.8115\n",
      "Epoch 186, Average Loss: 1.8065\n",
      "Epoch 187, Average Loss: 1.8146\n",
      "Epoch 188, Average Loss: 1.8143\n",
      "Epoch 189, Average Loss: 1.8069\n",
      "Epoch 190, Average Loss: 1.8049\n",
      "Epoch 191, Average Loss: 1.8063\n",
      "Epoch 192, Average Loss: 1.8049\n",
      "Epoch 193, Average Loss: 1.8085\n",
      "Epoch 194, Average Loss: 1.8051\n",
      "Epoch 195, Average Loss: 1.8097\n",
      "Epoch 196, Average Loss: 1.8046\n",
      "Epoch 197, Average Loss: 1.8061\n",
      "Epoch 198, Average Loss: 1.8050\n",
      "Epoch 199, Average Loss: 1.8012\n",
      "Epoch 200, Average Loss: 1.8067\n"
     ]
    }
   ],
   "source": [
    "# Assuming X is a torch.Tensor of shape (n, d)\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = Y1.size(1)   # Dimension of the input data\n",
    "hidden_dim = 100        # Size of the hidden layer\n",
    "latent_dim = 10         # Dimension of the latent space\n",
    "batch_size = 100        # Batch size for training\n",
    "learning_rate = 1e-3    # Learning rate\n",
    "epochs = 200             # Number of training epochs\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(Y1)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the VAE model\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data_batch in dataloader:\n",
    "        data = data_batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss = loss / batch_size\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * batch_size\n",
    "        optimizer.step()\n",
    "    average_loss = train_loss / len(dataset)\n",
    "    print(f'Epoch {epoch + 1}, Average Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0555, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(recon_batch, data, reduction='mean')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1430,  1.2515,  0.2960, -0.2840, -1.0920,  2.6523, -0.7730,  0.4186,\n",
       "          0.9832, -0.6907]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, input_dim, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_function(x):\n",
    "    mu, _ = model.encode(x)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Y1[0:1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.random.normal(0, 1, (n, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
