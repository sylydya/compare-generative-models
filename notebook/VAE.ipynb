{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.functional import jacobian\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_mvn(to, fr):\n",
    "    \"\"\"Calculate `KL(to||fr)`, where `to` and `fr` are pairs of means and covariance matrices\"\"\"\n",
    "    m_to, S_to = to\n",
    "    m_fr, S_fr = fr\n",
    "    \n",
    "    d = m_fr - m_to\n",
    "    \n",
    "    c, lower = scipy.linalg.cho_factor(S_fr)\n",
    "    def solve(B):\n",
    "        return scipy.linalg.cho_solve((c, lower), B)\n",
    "    \n",
    "    def logdet(S):\n",
    "        return np.linalg.slogdet(S)[1]\n",
    "\n",
    "    term1 = np.trace(solve(S_to))\n",
    "    term2 = logdet(S_fr) - logdet(S_to)\n",
    "    term3 = d.T @ solve(d)\n",
    "    return (term1 + term2 + term3 - len(d))/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)     # Outputs the mean\n",
    "        self.fc_short_encode = nn.Linear(input_dim, latent_dim)    # shortcut to the latent space\n",
    "\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim) # Outputs the log variance\n",
    "\n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.fc_short_decode = nn.Linear(latent_dim, input_dim)    # shortcut to the input space\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encodes input by mapping it into the latent space.\"\"\"\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h1) + self.fc_short_encode(x)\n",
    "        logvar = self.fc_logvar(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Applies the reparameterization trick to sample from N(mu, var).\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)   # Calculate the standard deviation\n",
    "        eps = torch.randn_like(std)     # Sample from standard normal\n",
    "        return mu + eps * std           # Sample from N(mu, var)\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Decodes the latent representation z to reconstruct the input.\"\"\"\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        # x_recon = torch.sigmoid(self.fc4(h3))\n",
    "        x_recon = self.fc4(h3) + self.fc_short_decode(z)\n",
    "\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the computation performed at every call.\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"Computes the VAE loss function.\"\"\"\n",
    "    # Reconstruction loss (binary cross entropy)\n",
    "    # BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "\n",
    "    # KL divergence between the approximate posterior and the prior\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) * 0.05\n",
    "\n",
    "    # KLD = 0\n",
    "\n",
    "    return MSE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n = 200000\n",
    "d = 10\n",
    "\n",
    "np.random.seed(seed)\n",
    "# A1_vector = np.random.normal(0, 1, d)\n",
    "# A2_vector = np.random.normal(0, 1, d)\n",
    "\n",
    "A1_vector = np.random.uniform(0.8, 1.2, d)\n",
    "A2_vector = np.random.uniform(0.8, 1.2, d)\n",
    "\n",
    "A1 = np.diag(A1_vector)\n",
    "A2 = np.diag(A2_vector)\n",
    "\n",
    "B1 = np.random.normal(0, 1, d)\n",
    "B2 = np.random.normal(0, 1, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "X = np.random.normal(0, 1, (n, d))\n",
    "Y1 = X * A1_vector + B1\n",
    "Y2 = X * A2_vector + B2\n",
    "Y = np.random.normal(0, 1, (n, d))\n",
    "\n",
    "\n",
    "Y1 = torch.from_numpy(Y1).float()\n",
    "Y2 = torch.from_numpy(Y2).float()\n",
    "Y = torch.from_numpy(Y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_inverse_true = (Y - B1) / A1_vector\n",
    "g2_inverse_true = (Y - B2) / A2_vector\n",
    "d1 = d\n",
    "d2 = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.5349, 36.4740, 12.3655,  ..., 12.2571, 12.9371, 20.3265],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1_inverse_true.pow(2).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11.6004, 29.1244, 11.9660,  ..., 16.4414, 12.7913,  7.2787],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2_inverse_true.pow(2).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = Y1.size(1)   # Dimension of the input data\n",
    "hidden_dim = 50        # Size of the hidden layer\n",
    "latent_dim = 10         # Dimension of the latent space\n",
    "batch_size = 100        # Batch size for training\n",
    "learning_rate = 5e-3    # Learning rate\n",
    "epochs = 50             # Number of training epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yansun/opt/anaconda3/envs/llmenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 1.3565\n",
      "Epoch 2, Average Loss: 1.1508\n",
      "Epoch 3, Average Loss: 1.1480\n",
      "Epoch 4, Average Loss: 1.1464\n",
      "Epoch 5, Average Loss: 1.1446\n",
      "Epoch 6, Average Loss: 1.1443\n",
      "Epoch 7, Average Loss: 1.1432\n",
      "Epoch 8, Average Loss: 1.1432\n",
      "Epoch 9, Average Loss: 1.1429\n",
      "Epoch 10, Average Loss: 1.1428\n",
      "Epoch 11, Average Loss: 1.1426\n",
      "Epoch 12, Average Loss: 1.1418\n",
      "Epoch 13, Average Loss: 1.1417\n",
      "Epoch 14, Average Loss: 1.1421\n",
      "Epoch 15, Average Loss: 1.1422\n",
      "Epoch 16, Average Loss: 1.1418\n",
      "Epoch 17, Average Loss: 1.1420\n",
      "Epoch 18, Average Loss: 1.1418\n",
      "Epoch 19, Average Loss: 1.1414\n",
      "Epoch 20, Average Loss: 1.1413\n",
      "Epoch 21, Average Loss: 1.1415\n",
      "Epoch 22, Average Loss: 1.1417\n",
      "Epoch 23, Average Loss: 1.1411\n",
      "Epoch 24, Average Loss: 1.1414\n",
      "Epoch 25, Average Loss: 1.1415\n",
      "Epoch 26, Average Loss: 1.1412\n",
      "Epoch 27, Average Loss: 1.1411\n",
      "Epoch 28, Average Loss: 1.1408\n",
      "Epoch 29, Average Loss: 1.1413\n",
      "Epoch 30, Average Loss: 1.1414\n",
      "Epoch 31, Average Loss: 1.1420\n",
      "Epoch 32, Average Loss: 1.1413\n",
      "Epoch 33, Average Loss: 1.1410\n",
      "Epoch 34, Average Loss: 1.1417\n",
      "Epoch 35, Average Loss: 1.1413\n",
      "Epoch 36, Average Loss: 1.1409\n",
      "Epoch 37, Average Loss: 1.1415\n",
      "Epoch 38, Average Loss: 1.1410\n",
      "Epoch 39, Average Loss: 1.1410\n",
      "Epoch 40, Average Loss: 1.1415\n",
      "Epoch 41, Average Loss: 1.1416\n",
      "Epoch 42, Average Loss: 1.1408\n",
      "Epoch 43, Average Loss: 1.1411\n",
      "Epoch 44, Average Loss: 1.1412\n",
      "Epoch 45, Average Loss: 1.1412\n",
      "Epoch 46, Average Loss: 1.1412\n",
      "Epoch 47, Average Loss: 1.1413\n",
      "Epoch 48, Average Loss: 1.1415\n",
      "Epoch 49, Average Loss: 1.1410\n",
      "Epoch 50, Average Loss: 1.1413\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "dataset1 = TensorDataset(Y1)\n",
    "dataloader1 = DataLoader(dataset1, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the VAE model\n",
    "model1 = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model1.train()\n",
    "    train_loss = 0\n",
    "    for data_batch in dataloader1:\n",
    "        data = data_batch[0]\n",
    "        optimizer1.zero_grad()\n",
    "        recon_batch, mu, logvar = model1(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss = loss / batch_size\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * batch_size\n",
    "        optimizer1.step()\n",
    "    average_loss = train_loss / len(dataset1)\n",
    "    print(f'Epoch {epoch + 1}, Average Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0278, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0228, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0145, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((Y - model1.decode(model1.encode(Y)[0])).pow(2).sum(1).mean())\n",
    "print((Y2 - model1.decode(model1.encode(Y2)[0])).pow(2).sum(1).mean())\n",
    "print((Y1 - model1.decode(model1.encode(Y1)[0])).pow(2).sum(1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.8003)\n",
      "tensor(9.9893, dtype=torch.float64)\n",
      "tensor(19.2514)\n",
      "tensor(19.9927, dtype=torch.float64)\n",
      "tensor(18.3882)\n",
      "tensor(19.0144, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(np.power(model1.encode(Y1)[0].data, 2).sum(1).mean())\n",
    "print(np.power((Y1 - B1) / A1_vector, 2).sum(1).mean())\n",
    "\n",
    "print(np.power(model1.encode(Y)[0].data, 2).sum(1).mean())\n",
    "print(np.power((Y - B1) / A1_vector, 2).sum(1).mean())\n",
    "\n",
    "print(np.power(model1.encode(Y2)[0].data, 2).sum(1).mean())\n",
    "print(np.power((Y2 - B1) / A1_vector, 2).sum(1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 1.3328\n",
      "Epoch 2, Average Loss: 1.1711\n",
      "Epoch 3, Average Loss: 1.1683\n",
      "Epoch 4, Average Loss: 1.1672\n",
      "Epoch 5, Average Loss: 1.1658\n",
      "Epoch 6, Average Loss: 1.1659\n",
      "Epoch 7, Average Loss: 1.1648\n",
      "Epoch 8, Average Loss: 1.1647\n",
      "Epoch 9, Average Loss: 1.1638\n",
      "Epoch 10, Average Loss: 1.1640\n",
      "Epoch 11, Average Loss: 1.1634\n",
      "Epoch 12, Average Loss: 1.1642\n",
      "Epoch 13, Average Loss: 1.1632\n",
      "Epoch 14, Average Loss: 1.1636\n",
      "Epoch 15, Average Loss: 1.1635\n",
      "Epoch 16, Average Loss: 1.1634\n",
      "Epoch 17, Average Loss: 1.1630\n",
      "Epoch 18, Average Loss: 1.1635\n",
      "Epoch 19, Average Loss: 1.1629\n",
      "Epoch 20, Average Loss: 1.1630\n",
      "Epoch 21, Average Loss: 1.1633\n",
      "Epoch 22, Average Loss: 1.1628\n",
      "Epoch 23, Average Loss: 1.1631\n",
      "Epoch 24, Average Loss: 1.1631\n",
      "Epoch 25, Average Loss: 1.1631\n",
      "Epoch 26, Average Loss: 1.1634\n",
      "Epoch 27, Average Loss: 1.1631\n",
      "Epoch 28, Average Loss: 1.1625\n",
      "Epoch 29, Average Loss: 1.1626\n",
      "Epoch 30, Average Loss: 1.1627\n",
      "Epoch 31, Average Loss: 1.1625\n",
      "Epoch 32, Average Loss: 1.1624\n",
      "Epoch 33, Average Loss: 1.1628\n",
      "Epoch 34, Average Loss: 1.1629\n",
      "Epoch 35, Average Loss: 1.1627\n",
      "Epoch 36, Average Loss: 1.1623\n",
      "Epoch 37, Average Loss: 1.1628\n",
      "Epoch 38, Average Loss: 1.1625\n",
      "Epoch 39, Average Loss: 1.1624\n",
      "Epoch 40, Average Loss: 1.1630\n",
      "Epoch 41, Average Loss: 1.1631\n",
      "Epoch 42, Average Loss: 1.1631\n",
      "Epoch 43, Average Loss: 1.1626\n",
      "Epoch 44, Average Loss: 1.1626\n",
      "Epoch 45, Average Loss: 1.1627\n",
      "Epoch 46, Average Loss: 1.1624\n",
      "Epoch 47, Average Loss: 1.1624\n",
      "Epoch 48, Average Loss: 1.1626\n",
      "Epoch 49, Average Loss: 1.1627\n",
      "Epoch 50, Average Loss: 1.1622\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "dataset2 = TensorDataset(Y2)\n",
    "dataloader2 = DataLoader(dataset2, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the VAE model\n",
    "model2 = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model2.train()\n",
    "    train_loss = 0\n",
    "    for data_batch in dataloader2:\n",
    "        data = data_batch[0]\n",
    "        optimizer2.zero_grad()\n",
    "        recon_batch, mu, logvar = model2(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss = loss / batch_size\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * batch_size\n",
    "        optimizer2.step()\n",
    "    average_loss = train_loss / len(dataset2)\n",
    "    print(f'Epoch {epoch + 1}, Average Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0226, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0163, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0341, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((Y - model2.decode(model2.encode(Y)[0])).pow(2).sum(1).mean())\n",
    "print((Y2 - model2.decode(model2.encode(Y2)[0])).pow(2).sum(1).mean())\n",
    "print((Y1 - model2.decode(model2.encode(Y1)[0])).pow(2).sum(1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.8971)\n",
      "tensor(9.9893, dtype=torch.float64)\n",
      "tensor(15.2488)\n",
      "tensor(15.5987, dtype=torch.float64)\n",
      "tensor(17.8243)\n",
      "tensor(18.0450, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(np.power(model2.encode(Y2)[0].data, 2).sum(1).mean())\n",
    "print(np.power((Y2 - B2) / A2_vector, 2).sum(1).mean())\n",
    "\n",
    "print(np.power(model2.encode(Y)[0].data, 2).sum(1).mean())\n",
    "print(np.power((Y - B2) / A2_vector, 2).sum(1).mean())\n",
    "\n",
    "print(np.power(model2.encode(Y1)[0].data, 2).sum(1).mean())\n",
    "print(np.power((Y1 - B2) / A2_vector, 2).sum(1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g1_inverse_g2_function(x):\n",
    "    mu, _ = model1.encode(model2.decode(x))\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "delta_list = []\n",
    "std_list = []\n",
    "for seed in range(100):\n",
    "    if seed % 10 == 0:\n",
    "        print(seed)\n",
    "    np.random.seed(seed)\n",
    "    # X = np.random.normal(0, 1, (n, d))\n",
    "    # Y1 = X * A1_vector + B1\n",
    "    # Y2 = X * A2_vector + B2\n",
    "    Y = np.random.normal(0, 1, (n, d))\n",
    "    Y = torch.from_numpy(Y).float()\n",
    "\n",
    "    # g1_inverse = (Y - B1) / A1_vector\n",
    "    # g2_inverse = (Y - B2) / A2_vector\n",
    "\n",
    "    g1_inverse = model1.encode(Y)[0].data\n",
    "    g2_inverse = model2.encode(Y)[0].data\n",
    "    d1 = d\n",
    "    d2 = d\n",
    "\n",
    "    log_jacobian_list = []\n",
    "    for index in range(num_samples):\n",
    "        # if index % 10 == 0:\n",
    "        #     print(index)\n",
    "        z = torch.clone(g2_inverse[index:index+1,].data)\n",
    "        z.requires_grad = True\n",
    "        J_g1_inverse_g2 = jacobian(g1_inverse_g2_function, z)\n",
    "        jacobian_g1_inverse_g2 = z.grad\n",
    "        log_jacobian_list.append(torch.log(torch.det(J_g1_inverse_g2[0,:,0,:]).abs()).item())\n",
    "\n",
    "\n",
    "    # delta_pp = np.log(np.abs(A2_vector / A1_vector)).sum() -0.5 * np.power(g1_inverse, 2).sum(1).mean() + 0.5 * np.power(g2_inverse, 2).sum(1).mean() + (d2 - d1) * np.log(2 * np.pi)\n",
    "    # std_pp = (-0.5 * np.power(g1_inverse, 2).sum(1) + 0.5 * np.power(g2_inverse, 2).sum(1)).std() / np.sqrt(n)\n",
    "\n",
    "    delta_pp = torch.FloatTensor(log_jacobian_list) -0.5 * np.power(g1_inverse, 2).sum(1)[0:num_samples]+ 0.5 * np.power(g2_inverse, 2).sum(1)[0:num_samples] + (d2 - d1) * np.log(2 * np.pi)\n",
    "    std_pp = delta_pp.std()\n",
    "    delta_pp = delta_pp.mean()\n",
    "\n",
    "    delta_list.append(delta_pp)\n",
    "    std_list.append(std_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.1964, dtype=torch.float64)\n",
      "tensor(-2.0011)\n"
     ]
    }
   ],
   "source": [
    "print(-0.5 * np.power((Y - B1) / A1_vector, 2).sum(1).mean() + 0.5 * np.power((Y - B2) / A2_vector, 2).sum(1).mean() + (d2 - d1) * np.log(2 * np.pi))\n",
    "print(-0.5 * np.power(g1_inverse, 2).sum(1).mean() + 0.5 * np.power(g2_inverse, 2).sum(1).mean() + (d2 - d1) * np.log(2 * np.pi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0759)\n",
      "0.42535272172772054\n"
     ]
    }
   ],
   "source": [
    "print(torch.FloatTensor(log_jacobian_list).mean())\n",
    "print(np.log(np.abs(A2_vector / A1_vector)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(torch.FloatTensor(log_jacobian_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.,  9.,  7., 17., 22., 15.,  9.,  9.,  3.,  4.]),\n",
       " array([-2.16974306, -2.12054729, -2.07135153, -2.02215576, -1.97295988,\n",
       "        -1.92376411, -1.87456834, -1.82537258, -1.77617669, -1.72698092,\n",
       "        -1.67778516]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histgoram of unnormalized statistics\n",
    "plt.hist(delta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7695912004473735\n"
     ]
    }
   ],
   "source": [
    "mu = np.zeros(d)\n",
    "sigma = np.diag(np.ones(d))\n",
    "\n",
    "dist = -kl_mvn((mu, sigma), (B1, np.power(A1, 2))) + kl_mvn((mu, sigma), (B2, np.power(A2, 2)))\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  9.,  9., 15., 24., 17.,  8., 10.,  3.,  4.]),\n",
       " array([-4.23204899, -3.71496224, -3.1978755 , -2.68078876, -2.16370201,\n",
       "        -1.64661515, -1.1295284 , -0.61244166, -0.09535489,  0.42173186,\n",
       "         0.93881863]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGfCAYAAAD/BbCUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAXmElEQVR4nO3dbYxU9d3w8d8AsoCyq4uwy8ZFEGttasUEBVFrQIlAG6zW2mhtXYyxasAEN62C0VKszVZ7RY12K33RgrZS7YNAlJY+oEIaRStKiDYQIRgW6a5PYRf2joNl537Ru3NfWxAZmPkPs34+yUmcM2fn/DxB9uuZM3MyuVwuFwAAifQr9wAAwKeL+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJIaUMjGLS0t8dRTT8WmTZti8ODBcd5558W9994bn/3sZ/PbTJ48OdasWdPr52688cZYtGjRIe2jp6cndu7cGUOHDo1MJlPIeABAmeRyudi9e3c0NDREv34HP7eRKeTeLtOnT4+rrroqzjnnnPjXv/4Vd9xxR7z++uvxj3/8I4499tiI+Hd8nHbaaXH33Xfnf27IkCFRXV19SPvYsWNHNDY2HupIAMBRpK2tLU466aSDblPQmY9Vq1b1erxkyZIYMWJErF+/Pi688ML8+iFDhkR9fX0hL503dOjQiPj38IcaLABAeXV1dUVjY2P+9/jBFBQf/62zszMiImpra3utf/zxx+NXv/pV1NfXx8yZM+Ouu+6KIUOGHPA1stlsZLPZ/OPdu3dHRER1dbX4AIAKcyiXTBx2fPT09MTcuXPj/PPPjzPOOCO//hvf+EacfPLJ0dDQEBs3bozbb789Nm/eHE899dQBX6elpSUWLlx4uGMAABWmoGs+/rebb745/vjHP8bf/va3g7638+yzz8bFF18cW7ZsibFjx+73/H+f+fjPaZvOzk5nPgCgQnR1dUVNTc0h/f4+rDMfc+bMiWeeeSbWrl37iReVTJw4MSLiY+OjqqoqqqqqDmcMAKACFRQfuVwubrnllli2bFk8//zzMWbMmE/8mQ0bNkRExMiRIw9rQACgbykoPmbPnh1Lly6NFStWxNChQ6O9vT0iImpqamLw4MGxdevWWLp0aXzpS1+KYcOGxcaNG+PWW2+NCy+8MM4888yS/AsAAJWloGs+Pu4K1sWLF8esWbOira0tvvnNb8brr78e3d3d0djYGJdffnnceeedh3z9RiHvGQEAR4eSXfPxSZ3S2Ni437ebAgD8b+7tAgAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASR32XW2B8hs9b2W5RyjYWz/6crlHAMrMmQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUgXFR0tLS5xzzjkxdOjQGDFiRFx22WWxefPmXtt8+OGHMXv27Bg2bFgcd9xxccUVV0RHR0dRhwYAKldB8bFmzZqYPXt2rFu3Lv7yl7/ERx99FJdcckl0d3fnt7n11lvj6aefjt/+9rexZs2a2LlzZ3z1q18t+uAAQGUaUMjGq1at6vV4yZIlMWLEiFi/fn1ceOGF0dnZGT//+c9j6dKlcdFFF0VExOLFi+Nzn/tcrFu3Ls4999ziTQ4AVKQjuuajs7MzIiJqa2sjImL9+vXx0UcfxdSpU/PbnH766TFq1Kh48cUXD/ga2Ww2urq6ei0AQN912PHR09MTc+fOjfPPPz/OOOOMiIhob2+PgQMHxvHHH99r27q6umhvbz/g67S0tERNTU1+aWxsPNyRAIAKcNjxMXv27Hj99dfjiSeeOKIB5s+fH52dnfmlra3tiF4PADi6FXTNx3/MmTMnnnnmmVi7dm2cdNJJ+fX19fWxd+/e2LVrV6+zHx0dHVFfX3/A16qqqoqqqqrDGQMAqEAFnfnI5XIxZ86cWLZsWTz77LMxZsyYXs+PHz8+jjnmmFi9enV+3ebNm2P79u0xadKk4kwMAFS0gs58zJ49O5YuXRorVqyIoUOH5q/jqKmpicGDB0dNTU1cf/310dzcHLW1tVFdXR233HJLTJo0ySddAICIKDA+HnnkkYiImDx5cq/1ixcvjlmzZkVExAMPPBD9+vWLK664IrLZbEybNi1++tOfFmVYAKDyFRQfuVzuE7cZNGhQtLa2Rmtr62EPBQD0Xe7tAgAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFIDyj0A8Okyet7Kco9QsLd+9OVyjwB9ijMfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBSBcfH2rVrY+bMmdHQ0BCZTCaWL1/e6/lZs2ZFJpPptUyfPr1Y8wIAFa7g+Oju7o5x48ZFa2vrx24zffr0+Oc//5lffv3rXx/RkABA3zGg0B+YMWNGzJgx46DbVFVVRX19/SG9XjabjWw2m3/c1dVV6EgAQAUpOD4OxfPPPx8jRoyIE044IS666KK45557YtiwYQfctqWlJRYuXFiKMaAgo+etLPcIAJ8KRb/gdPr06fHYY4/F6tWr49577401a9bEjBkzYt++fQfcfv78+dHZ2Zlf2traij0SAHAUKfqZj6uuuir/z1/4whfizDPPjLFjx8bzzz8fF1988X7bV1VVRVVVVbHHAACOUiX/qO0pp5wSJ554YmzZsqXUuwIAKkDJ42PHjh3x/vvvx8iRI0u9KwCgAhT8tsuePXt6ncXYtm1bbNiwIWpra6O2tjYWLlwYV1xxRdTX18fWrVvjtttui1NPPTWmTZtW1MEBgMpUcHy88sorMWXKlPzj5ubmiIhoamqKRx55JDZu3BiPPvpo7Nq1KxoaGuKSSy6JH/zgB67rAAAi4jDiY/LkyZHL5T72+T/96U9HNBAA0Le5twsAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AIKmC42Pt2rUxc+bMaGhoiEwmE8uXL+/1fC6Xi+9973sxcuTIGDx4cEydOjXefPPNYs0LAFS4guOju7s7xo0bF62trQd8/r777ouHHnooFi1aFC+99FIce+yxMW3atPjwww+PeFgAoPINKPQHZsyYETNmzDjgc7lcLh588MG488474ytf+UpERDz22GNRV1cXy5cvj6uuuurIpgUAKl5Rr/nYtm1btLe3x9SpU/PrampqYuLEifHiiy8e8Gey2Wx0dXX1WgCAvqvgMx8H097eHhERdXV1vdbX1dXln/tvLS0tsXDhwmKOAUBEjJ63stwjFOytH3253COQQNk/7TJ//vzo7OzML21tbeUeCQAooaLGR319fUREdHR09Frf0dGRf+6/VVVVRXV1da8FAOi7ihofY8aMifr6+li9enV+XVdXV7z00ksxadKkYu4KAKhQBV/zsWfPntiyZUv+8bZt22LDhg1RW1sbo0aNirlz58Y999wTn/nMZ2LMmDFx1113RUNDQ1x22WXFnBsAqFAFx8crr7wSU6ZMyT9ubm6OiIimpqZYsmRJ3HbbbdHd3R3f/va3Y9euXXHBBRfEqlWrYtCgQcWbGgCoWAXHx+TJkyOXy33s85lMJu6+++64++67j2gwAKBvKvunXQCATxfxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEiqqHe1pTQq8c6UAPBxnPkAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQ1oNwDABztRs9bWe4RoE9x5gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSKnp8fP/7349MJtNrOf3004u9GwCgQg0oxYt+/vOfj7/+9a//fycDSrIbAKAClaQKBgwYEPX19aV4aQCgwpXkmo8333wzGhoa4pRTTolrrrkmtm/f/rHbZrPZ6Orq6rUAAH1X0eNj4sSJsWTJkli1alU88sgjsW3btvjiF78Yu3fvPuD2LS0tUVNTk18aGxuLPRIAcBTJ5HK5XCl3sGvXrjj55JPj/vvvj+uvv36/57PZbGSz2fzjrq6uaGxsjM7Ozqiuri7laBVj9LyV5R4BIIm3fvTlco/AYerq6oqamppD+v1d8itBjz/++DjttNNiy5YtB3y+qqoqqqqqSj0GAHCUKPn3fOzZsye2bt0aI0eOLPWuAIAKUPT4+M53vhNr1qyJt956K1544YW4/PLLo3///nH11VcXe1cAQAUq+tsuO3bsiKuvvjref//9GD58eFxwwQWxbt26GD58eLF3BQBUoKLHxxNPPFHslwQA+hD3dgEAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQVMlvLAcAfVkl3nm83HcPduYDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApNzVFoCjRiXeIZbCOfMBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBSA8o9QGqj560s9wgA8KnmzAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkipZfLS2tsbo0aNj0KBBMXHixHj55ZdLtSsAoIKUJD6efPLJaG5ujgULFsSrr74a48aNi2nTpsU777xTit0BABWkJDeWu//+++OGG26I6667LiIiFi1aFCtXroxf/OIXMW/evF7bZrPZyGaz+cednZ0REdHV1VWK0aIn+39K8roAUClK8Tv2P6+Zy+U+eeNckWWz2Vz//v1zy5Yt67X+2muvzV166aX7bb9gwYJcRFgsFovFYukDS1tb2ye2QtHPfLz33nuxb9++qKur67W+rq4uNm3atN/28+fPj+bm5vzjnp6e+OCDD2LYsGGRyWSKPV5RdXV1RWNjY7S1tUV1dXW5x+lTHNvScnxLx7EtHce2dIpxbHO5XOzevTsaGho+cduSvO1SiKqqqqiqquq17vjjjy/PMIepurrafwgl4tiWluNbOo5t6Ti2pXOkx7ampuaQtiv6Bacnnnhi9O/fPzo6Onqt7+joiPr6+mLvDgCoMEWPj4EDB8b48eNj9erV+XU9PT2xevXqmDRpUrF3BwBUmJK87dLc3BxNTU1x9tlnx4QJE+LBBx+M7u7u/Kdf+oqqqqpYsGDBfm8bceQc29JyfEvHsS0dx7Z0Uh/bTC53KJ+JKdxPfvKT+PGPfxzt7e1x1llnxUMPPRQTJ04sxa4AgApSsvgAADgQ93YBAJISHwBAUuIDAEhKfAAASYmPIstms3HWWWdFJpOJDRs2lHucPuPSSy+NUaNGxaBBg2LkyJHxrW99K3bu3FnusSreW2+9Fddff32MGTMmBg8eHGPHjo0FCxbE3r17yz1an/DDH/4wzjvvvBgyZEjFfXPz0aa1tTVGjx4dgwYNiokTJ8bLL79c7pH6hLVr18bMmTOjoaEhMplMLF++PMl+xUeR3XbbbYf0vfYUZsqUKfGb3/wmNm/eHL///e9j69at8bWvfa3cY1W8TZs2RU9PT/zsZz+LN954Ix544IFYtGhR3HHHHeUerU/Yu3dvXHnllXHzzTeXe5SK9uSTT0Zzc3MsWLAgXn311Rg3blxMmzYt3nnnnXKPVvG6u7tj3Lhx0dramnbHRbiRLf/PH/7wh9zpp5+ee+ONN3IRkXvttdfKPVKftWLFilwmk8nt3bu33KP0Offdd19uzJgx5R6jT1m8eHGupqam3GNUrAkTJuRmz56df7xv375cQ0NDrqWlpYxT9T0Rsd8d6UvFmY8i6ejoiBtuuCF++ctfxpAhQ8o9Tp/2wQcfxOOPPx7nnXdeHHPMMeUep8/p7OyM2traco8BEfHvs0fr16+PqVOn5tf169cvpk6dGi+++GIZJ+NIiI8iyOVyMWvWrLjpppvi7LPPLvc4fdbtt98exx57bAwbNiy2b98eK1asKPdIfc6WLVvi4YcfjhtvvLHco0BERLz33nuxb9++qKur67W+rq4u2tvbyzQVR0p8HMS8efMik8kcdNm0aVM8/PDDsXv37pg/f365R64oh3p8/+O73/1uvPbaa/HnP/85+vfvH9dee23kfEHvARV6bCMi3n777Zg+fXpceeWVccMNN5Rp8qPf4RxboDdfr34Q7777brz//vsH3eaUU06Jr3/96/H0009HJpPJr9+3b1/0798/rrnmmnj00UdLPWpFOtTjO3DgwP3W79ixIxobG+OFF15wt+QDKPTY7ty5MyZPnhznnntuLFmyJPr18/8lH+dw/twuWbIk5s6dG7t27SrxdH3P3r17Y8iQIfG73/0uLrvssvz6pqam2LVrlzOgRZTJZGLZsmW9jnOplOSutn3F8OHDY/jw4Z+43UMPPRT33HNP/vHOnTtj2rRp8eSTT7qZ3kEc6vE9kJ6enoj490eb2V8hx/btt9+OKVOmxPjx42Px4sXC4xMcyZ9bCjdw4MAYP358rF69Ov9LsaenJ1avXh1z5swp73AcNvFRBKNGjer1+LjjjouIiLFjx8ZJJ51UjpH6lJdeein+/ve/xwUXXBAnnHBCbN26Ne66664YO3assx5H6O23347JkyfHySefHP/zP/8T7777bv65+vr6Mk7WN2zfvj0++OCD2L59e+zbty//3T+nnnpq/u8JPllzc3M0NTXF2WefHRMmTIgHH3wwuru747rrriv3aBVvz549sWXLlvzjbdu2xYYNG6K2tna/321FleQzNZ8y27Zt81HbItq4cWNuypQpudra2lxVVVVu9OjRuZtuuim3Y8eOco9W8RYvXpyLiAMuHLmmpqYDHtvnnnuu3KNVnIcffjg3atSo3MCBA3MTJkzIrVu3rtwj9QnPPffcAf+MNjU1lXS/rvkAAJLy5i4AkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS/xfJvR5B9QbNlQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histgoram of test statistics\n",
    "plt.hist((np.array(delta_list) - dist) / (np.array(std_list) / np.sqrt(num_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((Y - model1.decode(model1.encode(Y)[0])).pow(2).sum(1).mean())\n",
    "# print((Y2 - model1.decode(model1.encode(Y2)[0])).pow(2).sum(1).mean())\n",
    "# print((Y1 - model1.decode(model1.encode(Y1)[0])).pow(2).sum(1).mean())\n",
    "\n",
    "# print((Y - model2.decode(model2.encode(Y)[0])).pow(2).sum(1).mean())\n",
    "# print((Y2 - model2.decode(model2.encode(Y2)[0])).pow(2).sum(1).mean())\n",
    "# print((Y1 - model2.decode(model2.encode(Y1)[0])).pow(2).sum(1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
