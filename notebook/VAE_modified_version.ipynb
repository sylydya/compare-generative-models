{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.functional import jacobian\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_mvn(to, fr):\n",
    "    \"\"\"Calculate `KL(to||fr)`, where `to` and `fr` are pairs of means and covariance matrices\"\"\"\n",
    "    m_to, S_to = to\n",
    "    m_fr, S_fr = fr\n",
    "    \n",
    "    d = m_fr - m_to\n",
    "    \n",
    "    c, lower = scipy.linalg.cho_factor(S_fr)\n",
    "    def solve(B):\n",
    "        return scipy.linalg.cho_solve((c, lower), B)\n",
    "    \n",
    "    def logdet(S):\n",
    "        return np.linalg.slogdet(S)[1]\n",
    "\n",
    "    term1 = np.trace(solve(S_to))\n",
    "    term2 = logdet(S_fr) - logdet(S_to)\n",
    "    term3 = d.T @ solve(d)\n",
    "    return (term1 + term2 + term3 - len(d))/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class VAE(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "#         super(VAE, self).__init__()\n",
    "\n",
    "#         # Encoder layers\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         self.fc_mu = nn.Linear(hidden_dim, latent_dim)     # Outputs the mean\n",
    "#         self.fc_short_encode = nn.Linear(input_dim, latent_dim)    # shortcut to the latent space\n",
    "\n",
    "#         self.fc_logvar = nn.Linear(hidden_dim, latent_dim) # Outputs the log variance\n",
    "\n",
    "#         # Decoder layers\n",
    "#         self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "#         self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "#         self.fc_short_decode = nn.Linear(latent_dim, input_dim)    # shortcut to the input space\n",
    "\n",
    "#     def encode(self, x):\n",
    "#         \"\"\"Encodes input by mapping it into the latent space.\"\"\"\n",
    "#         h1 = F.relu(self.fc1(x))\n",
    "#         mu = self.fc_mu(h1) + self.fc_short_encode(x)\n",
    "#         logvar = self.fc_logvar(h1)\n",
    "#         return mu, logvar\n",
    "\n",
    "#     def reparameterize(self, mu, logvar):\n",
    "#         \"\"\"Applies the reparameterization trick to sample from N(mu, var).\"\"\"\n",
    "#         std = torch.exp(0.5 * logvar)   # Calculate the standard deviation\n",
    "#         eps = torch.randn_like(std)     # Sample from standard normal\n",
    "#         return mu + eps * std           # Sample from N(mu, var)\n",
    "\n",
    "#     def decode(self, z):\n",
    "#         \"\"\"Decodes the latent representation z to reconstruct the input.\"\"\"\n",
    "#         h3 = F.relu(self.fc3(z))\n",
    "#         # x_recon = torch.sigmoid(self.fc4(h3))\n",
    "#         x_recon = self.fc4(h3) + self.fc_short_decode(z)\n",
    "\n",
    "#         return x_recon\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"Defines the computation performed at every call.\"\"\"\n",
    "#         mu, logvar = self.encode(x)\n",
    "#         z = self.reparameterize(mu, logvar)\n",
    "#         x_recon = self.decode(z)\n",
    "#         return x_recon, mu, logvar\n",
    "\n",
    "# def loss_function(recon_x, x, mu, logvar):\n",
    "#     \"\"\"Computes the VAE loss function.\"\"\"\n",
    "#     # Reconstruction loss (binary cross entropy)\n",
    "#     # BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "#     MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "\n",
    "#     # KL divergence between the approximate posterior and the prior\n",
    "#     KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) * 0.05\n",
    "\n",
    "#     # KLD = 0\n",
    "\n",
    "#     return MSE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, latent_dim)     # Outputs the mean\n",
    "        self.fc_short_encode = nn.Linear(input_dim, latent_dim)    # shortcut to the latent space\n",
    "\n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.fc_short_decode = nn.Linear(latent_dim, input_dim)    # shortcut to the input space\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encodes input by mapping it into the latent space.\"\"\"\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        mu = self.fc2(h1) + self.fc_short_encode(x)\n",
    "        return mu\n",
    "\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Decodes the latent representation z to reconstruct the input.\"\"\"\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        # x_recon = torch.sigmoid(self.fc4(h3))\n",
    "        x_recon = self.fc4(h3) + self.fc_short_decode(z)\n",
    "\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the computation performed at every call.\"\"\"\n",
    "        mu = self.encode(x)\n",
    "        x_recon = self.decode(mu)\n",
    "        return x_recon, mu\n",
    "\n",
    "def loss_function(recon_x, x, mu, kl_lambda=0.05):\n",
    "    \"\"\"Computes the VAE loss function.\"\"\"\n",
    "    # Reconstruction loss (binary cross entropy)\n",
    "    # BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    MSE = F.mse_loss(recon_x, x, reduction='mean')\n",
    "\n",
    "    encode_mu = mu.mean(0)\n",
    "    encode_cov = torch.cov(mu.T)\n",
    "\n",
    "    # KL divergence between the approximate posterior and the prior\n",
    "    # KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) * 0.05\n",
    "    \n",
    "    KLD = kl_lambda * 0.5 * (torch.trace(encode_cov) + torch.dot(encode_mu, encode_mu) - len(encode_mu) - torch.logdet(encode_cov))\n",
    "\n",
    "    # KLD = 0\n",
    "\n",
    "    return MSE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n = 500000\n",
    "d = 10\n",
    "\n",
    "np.random.seed(seed)\n",
    "# A1_vector = np.random.normal(0, 1, d)\n",
    "# A2_vector = np.random.normal(0, 1, d)\n",
    "\n",
    "A1_vector = np.random.uniform(0.8, 1.2, d)\n",
    "A2_vector = np.random.uniform(0.8, 1.2, d)\n",
    "\n",
    "A1 = np.diag(A1_vector)\n",
    "A2 = np.diag(A2_vector)\n",
    "\n",
    "B1 = np.random.normal(0, 1, d)\n",
    "B2 = np.random.normal(0, 1, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "X = np.random.normal(0, 1, (n, d))\n",
    "Y1 = X * A1_vector + B1\n",
    "Y2 = X * A2_vector + B2\n",
    "Y = np.random.normal(0, 1, (n, d))\n",
    "\n",
    "\n",
    "Y1 = torch.from_numpy(Y1).float()\n",
    "Y2 = torch.from_numpy(Y2).float()\n",
    "Y = torch.from_numpy(Y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_inverse_true = (Y - B1) / A1_vector\n",
    "g2_inverse_true = (Y - B2) / A2_vector\n",
    "d1 = d\n",
    "d2 = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17.1204,  8.1893, 21.7644,  ..., 24.4307, 17.5329, 18.1688],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1_inverse_true.pow(2).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.4468,  8.2733, 17.9244,  ..., 14.6267, 12.2714,  8.6099],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2_inverse_true.pow(2).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = Y1.size(1)   # Dimension of the input data\n",
    "hidden_dim = 50        # Size of the hidden layer\n",
    "latent_dim = 10         # Dimension of the latent space\n",
    "batch_size = 1000        # Batch size for training\n",
    "learning_rate = 5e-3    # Learning rate\n",
    "epochs = 20             # Number of training epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create DataLoader\n",
    "# dataset1 = TensorDataset(Y1)\n",
    "# dataloader1 = DataLoader(dataset1, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Initialize the VAE model\n",
    "# model1 = VAE(input_dim, hidden_dim, latent_dim)\n",
    "# optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     model1.train()\n",
    "#     train_loss = 0\n",
    "#     for data_batch in dataloader1:\n",
    "#         data = data_batch[0]\n",
    "#         break\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recon_batch, mu = model1(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_mu = mu.mean(0)\n",
    "# encode_cov = torch.cov(mu.T)\n",
    "\n",
    "# 0.5 * (torch.trace(encode_cov) + torch.dot(encode_mu, encode_mu) - len(encode_mu) - torch.logdet(encode_cov))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yansun/opt/anaconda3/envs/llmenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.0528\n",
      "Epoch 2, Average Loss: 0.0028\n",
      "Epoch 3, Average Loss: 0.0024\n",
      "Epoch 4, Average Loss: 0.0021\n",
      "Epoch 5, Average Loss: 0.0021\n",
      "Epoch 6, Average Loss: 0.0020\n",
      "Epoch 7, Average Loss: 0.0020\n",
      "Epoch 8, Average Loss: 0.0020\n",
      "Epoch 9, Average Loss: 0.0020\n",
      "Epoch 10, Average Loss: 0.0020\n",
      "Epoch 11, Average Loss: 0.0020\n",
      "Epoch 12, Average Loss: 0.0020\n",
      "Epoch 13, Average Loss: 0.0020\n",
      "Epoch 14, Average Loss: 0.0020\n",
      "Epoch 15, Average Loss: 0.0020\n",
      "Epoch 16, Average Loss: 0.0020\n",
      "Epoch 17, Average Loss: 0.0020\n",
      "Epoch 18, Average Loss: 0.0020\n",
      "Epoch 19, Average Loss: 0.0020\n",
      "Epoch 20, Average Loss: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "dataset1 = TensorDataset(Y1)\n",
    "dataloader1 = DataLoader(dataset1, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the VAE model\n",
    "model1 = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model1.train()\n",
    "    train_loss = 0\n",
    "    for data_batch in dataloader1:\n",
    "        data = data_batch[0]\n",
    "        optimizer1.zero_grad()\n",
    "        recon_batch, mu = model1(data)\n",
    "        loss = loss_function(recon_batch, data, mu)\n",
    "        # loss = loss / batch_size\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * batch_size\n",
    "        optimizer1.step()\n",
    "    average_loss = train_loss / len(dataset1)\n",
    "    print(f'Epoch {epoch + 1}, Average Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_test = VAE(input_dim, hidden_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for para in model1_test.parameters():\n",
    "        para.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model1_test.fc_short_encode.weight.data = torch.FloatTensor(np.diag(np.power(A1_vector, -1)))\n",
    "    model1_test.fc_short_encode.bias.data = torch.FloatTensor(-B1 * np.power(A1_vector, -1))\n",
    "\n",
    "    model1_test.fc_short_decode.weight.data = torch.FloatTensor(np.diag(A1_vector))\n",
    "    model1_test.fc_short_decode.bias.data = torch.FloatTensor(B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model1_test.encode(Y1)[0])\n",
    "# print((Y1 - B1) / A1_vector)\n",
    "# print(model1_test.decode(model1_test.encode(Y1)[0]))\n",
    "# print(Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1: 0.2073, Loss1_test: 0.2157\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    data = Y2[0:100,]\n",
    "    recon_batch, mu = model1(data)\n",
    "    loss1 = loss_function(recon_batch, data, mu)\n",
    "    # print(F.mse_loss(recon_batch, data, reduction='sum'))\n",
    "    # print(-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) * 0.05)\n",
    "    # print(F.mse_loss(recon_batch, data, reduction='sum') + -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) * 0.05)\n",
    "    \n",
    "    recon_batch_test, mu_test = model1_test(data)\n",
    "    loss1_test = loss_function(recon_batch_test, data, mu_test)\n",
    "    # print(F.mse_loss(recon_batch_test, data, reduction='sum'))\n",
    "    # print(-0.5 * torch.sum(1 + logvar_test - mu_test.pow(2) - logvar_test.exp()) * 0.05)\n",
    "    # print(F.mse_loss(recon_batch_test, data, reduction='sum') + -0.5 * torch.sum(1 + logvar_test - mu_test.pow(2) - logvar_test.exp()) * 0.05)\n",
    "    print(f'Loss1: {loss1:.4f}, Loss1_test: {loss1_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2262, -0.4309, -0.1828,  0.3690,  0.0486,  0.2419, -0.4305, -0.3529,\n",
      "         0.0490,  0.0984])\n",
      "tensor([-1.1727,  1.0108,  0.2155,  0.9532, -0.0492, -0.6964,  1.2586, -1.2201,\n",
      "        -0.9404, -0.4948])\n"
     ]
    }
   ],
   "source": [
    "# print(model1.fc_short_encode.weight.data)\n",
    "print(model1.fc_short_encode.bias.data)\n",
    "# print(model1_test.fc_short_encode.weight.data)\n",
    "print(model1_test.fc_short_encode.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0843)\n",
      "tensor(-0.0636)\n",
      "tensor([[-1.6329e-27,  9.5454e-41,  2.2095e-01,  2.1066e-41, -4.7148e-41,\n",
      "         -9.6901e-41, -9.9872e-36,  1.1956e-40, -2.1482e-41, -2.5569e-25,\n",
      "         -1.1488e-12, -7.8642e-41, -1.0932e-40,  1.0590e-40, -5.7064e-41,\n",
      "         -1.2818e-37, -3.6352e-41, -1.5636e-41, -1.9454e-41,  4.4420e-09,\n",
      "          1.3991e-16,  3.4019e-41, -4.4302e-41,  1.2512e-40,  1.3668e-17,\n",
      "         -1.0342e-37, -5.2977e-41,  8.4692e-41,  1.0747e-40, -9.6942e-42,\n",
      "         -5.9114e-41, -6.9929e-41,  3.5957e-42, -5.7410e-41,  8.4261e-41,\n",
      "          1.1375e-40,  4.9937e-41,  4.7290e-41,  1.9001e-01, -2.8533e-41,\n",
      "          2.0037e-41,  6.6420e-41, -3.5369e-41,  2.5624e-41,  2.0088e-38,\n",
      "          4.0705e-41,  3.3544e-41,  5.5004e-03, -4.9903e-39, -1.0486e-41],\n",
      "        [ 1.6756e-27, -1.4808e-41, -3.5912e-01,  1.5344e-42,  9.2779e-41,\n",
      "          2.9245e-42, -6.9523e-36, -1.8684e-41,  2.5728e-41, -1.1183e-24,\n",
      "         -2.7593e-12,  1.0421e-40,  2.6564e-41, -1.4925e-41, -1.5468e-41,\n",
      "         -3.0629e-35, -7.4162e-41,  7.8604e-41, -9.8810e-41, -2.8130e-08,\n",
      "         -8.6802e-16, -6.4528e-41, -1.0907e-40,  1.0398e-40, -4.8336e-18,\n",
      "          1.6546e-37,  1.2125e-40, -7.6926e-41, -6.1727e-42, -4.4982e-41,\n",
      "         -7.0544e-41, -5.4738e-41,  5.6149e-41,  1.9474e-41,  2.6695e-42,\n",
      "         -3.0802e-41,  8.9023e-41, -2.1822e-41, -3.0714e-01,  3.1971e-41,\n",
      "          3.9079e-41, -2.0778e-41, -4.1829e-41, -1.1531e-40,  2.4073e-38,\n",
      "         -3.7231e-41,  1.0472e-40, -1.0071e-02, -4.0628e-39,  8.0362e-41],\n",
      "        [ 8.4790e-27, -9.4638e-41, -1.4732e-01, -6.1677e-41,  2.3779e-41,\n",
      "         -3.6043e-41,  1.0358e-35,  8.4133e-41, -7.7926e-41, -6.9332e-26,\n",
      "         -2.1731e-12,  4.7902e-41, -6.2317e-41, -1.4124e-41,  5.4480e-41,\n",
      "         -1.0282e-34, -1.8004e-41,  1.0009e-40,  5.4873e-41, -7.7538e-09,\n",
      "         -1.8054e-15, -5.8728e-41, -3.0838e-41,  3.8754e-41,  1.8668e-17,\n",
      "         -2.8920e-37,  6.6866e-41,  3.8796e-41,  2.1391e-41, -2.1352e-41,\n",
      "          4.5688e-41,  1.7024e-41,  6.4998e-41,  4.5034e-41, -1.3094e-40,\n",
      "          1.2209e-40, -5.0270e-41, -1.3554e-40, -1.2417e-01, -5.0867e-42,\n",
      "         -1.2180e-41, -1.3688e-41, -2.5967e-41,  8.4508e-41, -2.6322e-38,\n",
      "         -6.0522e-41, -1.2502e-40, -5.2748e-03, -8.6785e-39, -1.2598e-41],\n",
      "        [ 5.0484e-27,  5.4296e-41,  2.9684e-01,  7.6410e-41,  1.1617e-40,\n",
      "          2.6997e-41,  5.9629e-36, -1.2782e-40,  5.1487e-41,  1.7161e-24,\n",
      "          1.9760e-12, -1.1184e-41,  9.4036e-41, -9.4069e-42, -3.5366e-41,\n",
      "          7.9665e-36,  7.8143e-41,  3.1260e-41, -5.1868e-41,  2.1108e-08,\n",
      "         -1.1990e-15,  2.1706e-42,  1.2907e-40,  3.6226e-41,  3.9675e-18,\n",
      "          6.6069e-38, -1.2717e-40,  1.7997e-41,  6.8364e-41,  4.4835e-41,\n",
      "         -3.6644e-42, -1.9534e-41, -7.2912e-41,  7.9497e-41,  3.1330e-41,\n",
      "         -7.3998e-41,  2.2084e-42,  1.0342e-40,  2.5195e-01,  5.4945e-42,\n",
      "          6.4914e-41, -1.2643e-41, -8.9305e-42,  3.8028e-41,  1.4344e-38,\n",
      "          1.2841e-41,  2.9776e-41,  6.3866e-03,  4.2450e-39, -8.5944e-41],\n",
      "        [-2.6046e-27,  2.4081e-41,  2.0303e-02,  1.7960e-41,  5.3642e-42,\n",
      "         -2.0013e-41, -8.5352e-37,  1.5957e-41, -1.0512e-40, -2.3312e-24,\n",
      "          1.1992e-12,  7.2896e-41, -1.5435e-41, -2.2365e-42,  9.3737e-41,\n",
      "         -4.3305e-36,  9.5204e-42, -2.7646e-41,  7.4916e-41,  1.7037e-08,\n",
      "          2.9585e-16, -1.0405e-40,  8.3555e-41,  4.6052e-41,  9.8720e-18,\n",
      "          1.1256e-37,  6.8650e-42, -7.1916e-41,  5.5931e-41, -8.1813e-41,\n",
      "          2.0019e-41, -4.5963e-43, -8.5549e-41, -8.5283e-42, -2.9035e-41,\n",
      "          7.3556e-41, -3.4766e-41, -1.7502e-42,  1.4289e-02,  1.9653e-41,\n",
      "         -3.5922e-41, -8.0247e-41, -5.5248e-41, -1.5103e-41,  6.1759e-40,\n",
      "          6.0532e-41,  4.6900e-41,  3.2068e-04, -2.4981e-39,  9.2779e-41],\n",
      "        [ 2.9847e-27,  4.9661e-41,  2.1463e-01,  9.4484e-41,  7.4325e-41,\n",
      "          4.4086e-41,  8.3546e-37, -1.0610e-40, -7.4947e-41,  1.5936e-24,\n",
      "          2.5866e-12,  1.1006e-40, -7.4912e-41,  2.5278e-41,  4.5719e-41,\n",
      "          6.9939e-35,  1.1048e-40,  5.9397e-41,  1.0825e-40,  1.3779e-08,\n",
      "         -3.2201e-16,  9.1126e-42,  1.3443e-40, -6.2054e-41,  2.0256e-17,\n",
      "          2.4006e-37,  8.0998e-41,  5.7805e-41,  6.7302e-41, -5.4511e-41,\n",
      "         -3.2182e-41,  1.9479e-41, -9.8616e-41, -2.9185e-41, -1.0539e-41,\n",
      "          9.0101e-41, -3.4208e-41, -7.9737e-41,  1.8277e-01, -3.5526e-41,\n",
      "          5.5517e-41, -3.3183e-41,  9.5396e-41, -2.2836e-41, -7.3384e-39,\n",
      "          2.5634e-41,  1.3431e-40,  6.8458e-03, -1.8857e-39,  1.1762e-40],\n",
      "        [-8.6053e-27, -2.7764e-41, -3.4678e-01,  1.4415e-41, -6.1769e-41,\n",
      "          6.8958e-42, -2.0048e-36,  8.1331e-41, -4.3622e-41, -4.5633e-25,\n",
      "         -4.8420e-12, -3.7566e-41,  7.5879e-41, -5.7687e-41, -7.1955e-41,\n",
      "         -9.3877e-35, -5.1799e-41, -7.5686e-41,  6.7758e-41, -2.6776e-08,\n",
      "         -1.4152e-15, -1.1994e-40,  4.5661e-41, -4.0390e-41, -1.0772e-17,\n",
      "         -5.1396e-38, -7.1629e-41, -1.1209e-40, -5.4369e-41, -6.4062e-41,\n",
      "         -1.9976e-41, -8.8397e-41, -6.4594e-41, -1.3560e-40, -4.6052e-41,\n",
      "          1.0173e-40, -6.0763e-41, -1.8574e-41, -2.9978e-01, -3.7275e-43,\n",
      "         -3.4672e-41,  1.2830e-41, -7.4535e-41, -3.3129e-41,  4.5490e-38,\n",
      "         -8.4041e-41,  5.6699e-41, -9.2041e-03, -1.6878e-39, -4.7284e-41],\n",
      "        [ 2.1429e-27,  3.7814e-41, -2.9605e-01, -6.4590e-41, -6.1350e-41,\n",
      "          3.1667e-41,  9.8516e-36, -1.2703e-40,  4.2424e-41, -4.4898e-25,\n",
      "         -1.3702e-12,  1.5227e-41,  1.8085e-41,  9.9084e-41, -1.2762e-40,\n",
      "         -5.8460e-35,  8.6334e-42, -1.0276e-41, -3.8527e-41, -2.6470e-08,\n",
      "         -5.8693e-16, -2.5588e-42,  8.7738e-41,  1.4433e-41,  1.0339e-18,\n",
      "         -1.0519e-37,  3.4134e-41,  1.2255e-40, -1.2062e-40, -6.1104e-41,\n",
      "          6.5172e-41,  5.1383e-41,  6.9640e-41,  1.0612e-40,  9.2484e-41,\n",
      "         -3.4678e-41,  7.8511e-41,  2.6556e-41, -2.5332e-01, -6.3899e-42,\n",
      "         -6.3072e-42, -3.5286e-41, -7.4899e-41,  4.0938e-41,  2.5367e-38,\n",
      "         -1.2266e-40, -8.5476e-41, -8.6489e-03, -6.5417e-40,  4.4211e-41],\n",
      "        [-5.1143e-28,  3.6348e-41,  2.1363e-02, -1.2962e-42, -2.4202e-41,\n",
      "         -1.8651e-42, -8.3507e-36, -1.0364e-40,  5.0541e-41, -2.4839e-24,\n",
      "         -1.5078e-13, -6.6695e-41,  9.4907e-41, -1.0108e-40,  1.3091e-40,\n",
      "          7.5651e-35, -6.9310e-41,  8.5552e-41, -1.0194e-40, -7.3327e-10,\n",
      "         -1.5182e-17, -2.6745e-41,  8.8306e-41, -1.3311e-41,  5.1332e-18,\n",
      "         -2.9410e-37, -8.4044e-41, -3.8229e-41, -1.0878e-40,  1.7754e-42,\n",
      "          1.1414e-41, -5.8099e-41,  7.4588e-41, -7.1601e-41, -1.0193e-40,\n",
      "          1.0970e-40,  9.1477e-41, -4.2207e-41,  2.1030e-02, -4.2169e-41,\n",
      "          2.1258e-41,  2.7139e-41, -8.7021e-41, -8.6036e-41, -8.3276e-39,\n",
      "          7.6434e-41, -4.4945e-41,  1.9334e-04,  6.6942e-40, -2.6919e-41],\n",
      "        [ 8.9865e-27,  2.2040e-41,  5.9879e-02, -7.9650e-42,  1.3877e-41,\n",
      "          1.0590e-41,  4.8997e-36,  1.2595e-40, -1.1150e-40, -2.8919e-25,\n",
      "          2.2114e-12,  4.1656e-41, -7.2981e-41, -1.2557e-41, -9.6690e-42,\n",
      "          5.7692e-35,  2.3976e-42, -1.1568e-41, -7.2818e-41,  1.1280e-08,\n",
      "         -1.4028e-15,  1.2336e-41, -9.7048e-41,  4.6439e-42,  8.3097e-19,\n",
      "         -3.0512e-38, -1.1182e-40,  1.4604e-41, -4.6463e-41,  9.2809e-41,\n",
      "         -7.2097e-42,  2.0378e-41, -3.9802e-41,  4.0247e-41,  6.1090e-41,\n",
      "          1.0957e-40, -1.0695e-41,  1.5627e-41,  5.3145e-02, -6.5215e-41,\n",
      "         -1.0390e-40,  4.6958e-41,  5.8763e-41,  6.4241e-41, -8.0084e-39,\n",
      "          6.8267e-41,  6.9956e-41,  2.2507e-03, -1.4190e-40,  9.2165e-41]])\n",
      "tensor([ 0.2262, -0.4309, -0.1828,  0.3690,  0.0486,  0.2419, -0.4305, -0.3529,\n",
      "         0.0490,  0.0984])\n"
     ]
    }
   ],
   "source": [
    "print(model1.fc1.weight.data.max())\n",
    "print(model1.fc1.weight.data.min())\n",
    "print(model1.fc2.weight.data)\n",
    "print(model1.fc2.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0179, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((Y - model1.decode(model1.encode(Y))).pow(2).sum(1).mean())\n",
    "print((Y2 - model1.decode(model1.encode(Y2))).pow(2).sum(1).mean())\n",
    "print((Y1 - model1.decode(model1.encode(Y1))).pow(2).sum(1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.9484)\n",
      "tensor(9.9927, dtype=torch.float64)\n",
      "tensor(19.2384)\n",
      "tensor(19.9754, dtype=torch.float64)\n",
      "tensor(18.5883)\n",
      "tensor(19.0084, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(np.power(model1.encode(Y1).data, 2).sum(1).mean())\n",
    "print(np.power((Y1 - B1) / A1_vector, 2).sum(1).mean())\n",
    "\n",
    "print(np.power(model1.encode(Y).data, 2).sum(1).mean())\n",
    "print(np.power((Y - B1) / A1_vector, 2).sum(1).mean())\n",
    "\n",
    "print(np.power(model1.encode(Y2).data, 2).sum(1).mean())\n",
    "print(np.power((Y2 - B1) / A1_vector, 2).sum(1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.0538\n",
      "Epoch 2, Average Loss: 0.0027\n",
      "Epoch 3, Average Loss: 0.0021\n",
      "Epoch 4, Average Loss: 0.0020\n",
      "Epoch 5, Average Loss: 0.0019\n",
      "Epoch 6, Average Loss: 0.0020\n",
      "Epoch 7, Average Loss: 0.0020\n",
      "Epoch 8, Average Loss: 0.0019\n",
      "Epoch 9, Average Loss: 0.0020\n",
      "Epoch 10, Average Loss: 0.0020\n",
      "Epoch 11, Average Loss: 0.0020\n",
      "Epoch 12, Average Loss: 0.0020\n",
      "Epoch 13, Average Loss: 0.0020\n",
      "Epoch 14, Average Loss: 0.0020\n",
      "Epoch 15, Average Loss: 0.0020\n",
      "Epoch 16, Average Loss: 0.0020\n",
      "Epoch 17, Average Loss: 0.0019\n",
      "Epoch 18, Average Loss: 0.0020\n",
      "Epoch 19, Average Loss: 0.0019\n",
      "Epoch 20, Average Loss: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "dataset2 = TensorDataset(Y2)\n",
    "dataloader2 = DataLoader(dataset2, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the VAE model\n",
    "model2 = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model2.train()\n",
    "    train_loss = 0\n",
    "    for data_batch in dataloader2:\n",
    "        data = data_batch[0]\n",
    "        optimizer2.zero_grad()\n",
    "        recon_batch, mu = model2(data)\n",
    "        loss = loss_function(recon_batch, data, mu)\n",
    "        # loss = loss / batch_size\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * batch_size\n",
    "        optimizer2.step()\n",
    "    average_loss = train_loss / len(dataset2)\n",
    "    print(f'Epoch {epoch + 1}, Average Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0142, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((Y - model2.decode(model2.encode(Y))).pow(2).sum(1).mean())\n",
    "print((Y2 - model2.decode(model2.encode(Y2))).pow(2).sum(1).mean())\n",
    "print((Y1 - model2.decode(model2.encode(Y1))).pow(2).sum(1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.9333)\n",
      "tensor(9.9927, dtype=torch.float64)\n",
      "tensor(15.2222)\n",
      "tensor(15.5861, dtype=torch.float64)\n",
      "tensor(17.9734)\n",
      "tensor(18.0592, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(np.power(model2.encode(Y2).data, 2).sum(1).mean())\n",
    "print(np.power((Y2 - B2) / A2_vector, 2).sum(1).mean())\n",
    "\n",
    "print(np.power(model2.encode(Y).data, 2).sum(1).mean())\n",
    "print(np.power((Y - B2) / A2_vector, 2).sum(1).mean())\n",
    "\n",
    "print(np.power(model2.encode(Y1).data, 2).sum(1).mean())\n",
    "print(np.power((Y1 - B2) / A2_vector, 2).sum(1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g1_inverse_g2_function(x):\n",
    "    mu = model1.encode(model2.decode(x))\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n"
     ]
    }
   ],
   "source": [
    "delta_list = []\n",
    "std_list = []\n",
    "for seed in range(500):\n",
    "    if seed % 10 == 0:\n",
    "        print(seed)\n",
    "    np.random.seed(seed)\n",
    "    # X = np.random.normal(0, 1, (n, d))\n",
    "    # Y1 = X * A1_vector + B1\n",
    "    # Y2 = X * A2_vector + B2\n",
    "    Y = np.random.normal(0, 1, (n, d))\n",
    "    Y = torch.from_numpy(Y).float()\n",
    "\n",
    "    # g1_inverse = (Y - B1) / A1_vector\n",
    "    # g2_inverse = (Y - B2) / A2_vector\n",
    "\n",
    "    g1_inverse = model1.encode(Y).data\n",
    "    g2_inverse = model2.encode(Y).data\n",
    "    d1 = d\n",
    "    d2 = d\n",
    "\n",
    "    log_jacobian_list = []\n",
    "    for index in range(num_samples):\n",
    "        # if index % 10 == 0:\n",
    "        #     print(index)\n",
    "        z = torch.clone(g2_inverse[index:index+1,].data)\n",
    "        z.requires_grad = True\n",
    "        J_g1_inverse_g2 = jacobian(g1_inverse_g2_function, z)\n",
    "        jacobian_g1_inverse_g2 = z.grad\n",
    "        log_jacobian_list.append(torch.log(torch.det(J_g1_inverse_g2[0,:,0,:]).abs()).item())\n",
    "\n",
    "\n",
    "    # delta_pp = np.log(np.abs(A2_vector / A1_vector)).sum() -0.5 * np.power(g1_inverse, 2).sum(1).mean() + 0.5 * np.power(g2_inverse, 2).sum(1).mean() + (d2 - d1) * np.log(2 * np.pi)\n",
    "    # std_pp = (-0.5 * np.power(g1_inverse, 2).sum(1) + 0.5 * np.power(g2_inverse, 2).sum(1)).std() / np.sqrt(n)\n",
    "\n",
    "    delta_pp = torch.FloatTensor(log_jacobian_list) -0.5 * np.power(g1_inverse, 2).sum(1)[0:num_samples]+ 0.5 * np.power(g2_inverse, 2).sum(1)[0:num_samples] + (d2 - d1) * np.log(2 * np.pi)\n",
    "    std_pp = delta_pp.std()\n",
    "    delta_pp = delta_pp.mean()\n",
    "\n",
    "    delta_list.append(delta_pp)\n",
    "    std_list.append(std_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.1886, dtype=torch.float64)\n",
      "tensor(-2.0017)\n"
     ]
    }
   ],
   "source": [
    "# compare g_inverse norm  with the true g_inverse norm\n",
    "\n",
    "print(-0.5 * np.power((Y - B1) / A1_vector, 2).sum(1).mean() + 0.5 * np.power((Y - B2) / A2_vector, 2).sum(1).mean() + (d2 - d1) * np.log(2 * np.pi))\n",
    "print(-0.5 * np.power(g1_inverse, 2).sum(1).mean() + 0.5 * np.power(g2_inverse, 2).sum(1).mean() + (d2 - d1) * np.log(2 * np.pi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2976)\n",
      "0.42535272172772054\n"
     ]
    }
   ],
   "source": [
    "# compare the log jacobian with the true log jacobian\n",
    "\n",
    "print(torch.FloatTensor(log_jacobian_list).mean())\n",
    "print(np.log(np.abs(A2_vector / A1_vector)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_jacobian_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(torch.FloatTensor(log_jacobian_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  4.,  20.,  38.,  51.,  97., 119.,  76.,  48.,  28.,  19.]),\n",
       " array([-1.99994206, -1.94664669, -1.89335144, -1.84005606, -1.78676081,\n",
       "        -1.73346543, -1.68017006, -1.6268748 , -1.57357943, -1.52028418,\n",
       "        -1.4669888 ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histgoram of unnormalized statistics\n",
    "plt.hist(delta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7695912004473735\n"
     ]
    }
   ],
   "source": [
    "mu = np.zeros(d)\n",
    "sigma = np.diag(np.ones(d))\n",
    "\n",
    "dist = -kl_mvn((mu, sigma), (B1, np.power(A1, 2))) + kl_mvn((mu, sigma), (B2, np.power(A2, 2)))\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7150152"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(delta_list).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  5.,  20.,  36.,  48.,  97., 120.,  81.,  45.,  29.,  19.]),\n",
       " array([-2.43245411, -1.873353  , -1.3142519 , -0.75515085, -0.19604978,\n",
       "         0.3630513 ,  0.9221524 ,  1.4812535 ,  2.04035449,  2.5994556 ,\n",
       "         3.1585567 ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histgoram of test statistics\n",
    "plt.hist((np.array(delta_list) - dist) / (np.array(std_list) / np.sqrt(num_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((Y - model1.decode(model1.encode(Y)[0])).pow(2).sum(1).mean())\n",
    "# print((Y2 - model1.decode(model1.encode(Y2)[0])).pow(2).sum(1).mean())\n",
    "# print((Y1 - model1.decode(model1.encode(Y1)[0])).pow(2).sum(1).mean())\n",
    "\n",
    "# print((Y - model2.decode(model2.encode(Y)[0])).pow(2).sum(1).mean())\n",
    "# print((Y2 - model2.decode(model2.encode(Y2)[0])).pow(2).sum(1).mean())\n",
    "# print((Y1 - model2.decode(model2.encode(Y1)[0])).pow(2).sum(1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
